<?xml version="1.0" encoding="utf-8"?>
<documentation>
  <language culture="en" />
  <introduction lang="en">
    Huffman's algorithm is a lossless data compression algorithm. It produces an optimal prefix-free code of variable length, based on the
    frequencies of characters in the input data (more frequent characters will be represented with less bits and vice versa). It was developed
    by David A. Huffman while he was a Ph.D. student at MIT. Today it is used in other compression algorithms such as DEFLATE and PKZIP, as well as
    some multimedia codecs like JPEG and MP3.
    <newline /><newline /><section headline="Significance of compression in cryptography"><newline />
      The only absolutely secure cipher is the one-time pad, assuming the following conditions are fulfilled:      
      <enum><item>The key is truly random</item><item>It is as long as the plaintext</item><item>It is used only once</item><item>It is kept secret</item></enum>      
      Given the difficulty of implementing such a system, certain compromises had to be made. With modern ciphers, where we traded absolute secrecy for
      practicality, the key is pseudorandom and is shorter than the plaintext. Such cipher is computationaly secure, but theoretically it is possible
      to break it. Unicity distance is a theoretical measure of the amount of ciphertext needed to unambiguously determine the correct key and is
      inversely proportional to plaintext redundancy. By compressing data prior to encryption, we increase unicity distance, as well as reduce the
      amount of data to be encrypted. Unicity distance of one-time pad is infinite.
    </section></introduction>
  <usage lang="en">
    You can compress anything using 'Binary' presentation format (it uses code page 437, an eight-bit character set), but you may get worse results
    than compressing data in its original form (for e.g. decoding UTF-32 encoded text to code page 437 will result in 4 times longer text).
    <newline /><newline /><section headline="Compression"><newline />
      Huffman compression involves the following steps:      
      <enum><item>Calculate character frequencies</item><item>Create a node for each characacter-frequency pair and add it to a priority queue</item><item>Combine two nodes with smallest frequencies until there is only one node left - the Huffman tree</item><item>Create a code table by traversing the tree to each character and note down the paths taken (0 for left turn, 1 for rigth turn)</item><item>Represent characters using their newly created codewords</item></enum>      
      Below is an example tree generated from the sentence "this is an example of a huffman tree":
      <newline /><img src="Huffman/DetailedDescription/example_tree.png"></img><newline /><newline />
      Note that compressing small amounts of data and/or high entropy data will result in a bigger compressed size. That is because we need to have
      the tree in order to be able to decompress, so it is prepended to the compressed data. Also, lossless data compression relies on redundancy
      in order to be effective. That fact is the reason why we do compression before encryption (since good encryption should produce seemingly random
      ciphertext).
      <newline /><newline /></section><section headline="Decompression"><newline />
      Process of decompression is simply a matter of reading compressed data like a "road map" to the appropriate characters in the tree. It involves
      the following steps:
      <enum><item>Start from the root node</item><item>Read next bit from the compressed data and go left if its 0 - otherwise go right</item><item>Repeat step 2 until you hit a leaf node and output the character contained in it</item><item>Repeat steps 1-3 until you've read all compressed data</item></enum></section></usage>
  <presentation lang="en">
    In the presentation view, you can see general information about the effectiveness of compression, as well as code table used in the process.
  </presentation>
  <references>
    <linkReference>
      <link lang="en" url="https://en.wikipedia.org/wiki/Huffman_coding" />
      <caption lang="en">Huffman coding (Wikipedia)</caption>
      <link lang="de-DE" url="https://de.wikipedia.org/wiki/Huffman-Kodierung" />
      <caption lang="de-DE">Huffman-Kodierung (Wikipedia)</caption>
    </linkReference>
    <linkReference>
      <link lang="en" url="https://en.wikipedia.org/wiki/Unicity_distance" />
      <caption lang="en">Unicity distance (Wikipedia)</caption>
    </linkReference>
  </references>
</documentation>